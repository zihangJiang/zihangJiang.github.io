---
title: "Vision permutator: A permutable mlp-like architecture for visual recognition"
collection: publications
permalink: /publication/2021-06-23-vip
excerpt: 'Introduce a new vision permutator module and ViP model for ImageNet classification tasks.'
date: 2021-06-23
venue: 'TPAMI'
paperurl: 'https://arxiv.org/pdf/2106.12368.pdf'
citation: 'Hou, Qibin, et al. "Vision permutator: A permutable mlp-like architecture for visual recognition." arXiv preprint arXiv:2106.12368 (2021).'
---
In this paper, we present Vision Permutator, a conceptually simple and data efficient MLP-like architecture for visual recognition. By realizing the importance of the positional information carried by 2D feature representations, unlike recent MLP-like models that encode the spatial information along the flattened spatial dimensions, Vision Permutator separately encodes the feature representations along the height and width dimensions with linear projections. This allows Vision Permutator to capture long-range dependencies along one spatial direction and meanwhile preserve precise positional information along the other direction. The resulting position-sensitive outputs are then aggregated in a mutually complementing manner to form expressive representations of the objects of interest. We show that our Vision Permutators are formidable competitors to convolutional neural networks (CNNs) and vision transformers. Without the dependence on spatial convolutions or attention mechanisms, Vision Permutator achieves 81.5% top-1 accuracy on ImageNet without extra large-scale training data (e.g., ImageNet-22k) using only 25M learnable parameters, which is much better than most CNNs and vision transformers under the same model size constraint. When scaling up to 88M, it attains 83.2% top-1 accuracy. We hope this work could encourage research on rethinking the way of encoding spatial information and facilitate the development of MLP-like models. Code is available at https://github.com/Andrew-Qibin/VisionPermutator.

[Download paper here](https://arxiv.org/pdf/2106.12368.pdf)
